{"metadata":{"colab":{"provenance":[],"collapsed_sections":["_leEuhPQUiwC"],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# 1. Setup","metadata":{"id":"_leEuhPQUiwC"}},{"cell_type":"markdown","source":"Import libraries","metadata":{"id":"iHxLSAJakHXy"}},{"cell_type":"code","source":"\n!pip install torch\n!pip install transformers","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_dpRJkj0WPxU","outputId":"0e0f6d72-453f-4c09-d21f-23a6f8892ef4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n\nRequirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n\nRequirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n\nRequirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n\nCollecting transformers\n\n  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n\nCollecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n\n  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n\nCollecting tokenizers<0.15,>=0.14 (from transformers)\n\n  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n\n  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n\nCollecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n\n  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n\nInstalling collected packages: safetensors, huggingface-hub, tokenizers, transformers\n\nSuccessfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n"}]},{"cell_type":"code","source":"import pandas as pd\nimport json\nimport random,operator\nimport numpy as np\nimport os, re, string\nimport torch\nimport re\nfrom sklearn.decomposition import TruncatedSVD\nimport torch.nn as nn\nimport logging\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom transformers import AdamW, get_linear_schedule_with_warmup, RobertaModel,PhobertTokenizer","metadata":{"id":"CmyovMvbjyXU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA version:\", torch.version.cuda)\n!nvcc --version","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdNZ3_BvclxY","outputId":"165673f1-cdf6-40a3-a49d-5c5dc974598e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"PyTorch version: 2.1.0+cu118\n\nCUDA version: 11.8\n\nnvcc: NVIDIA (R) Cuda compiler driver\n\nCopyright (c) 2005-2022 NVIDIA Corporation\n\nBuilt on Wed_Sep_21_10:33:58_PDT_2022\n\nCuda compilation tools, release 11.8, V11.8.89\n\nBuild cuda_11.8.r11.8/compiler.31833905_0\n"}]},{"cell_type":"code","source":"seed = 42\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\nrandom.seed(seed)\nnp.random.seed(seed)\nos.environ['PYTHONHASHSEED'] = str(seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False","metadata":{"id":"88OwemALLc_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(\"Device: \", device)\n!nvidia-smi","metadata":{"id":"vSe881ylLj90","colab":{"base_uri":"https://localhost:8080/"},"outputId":"00edde42-01db-424d-89c8-446fd79afeaf"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Device:  cpu\n\n/bin/bash: line 1: nvidia-smi: command not found\n"}]},{"cell_type":"code","source":"# connect gg drive\nfrom google.colab import drive\ndrive.mount('/content/gdrive')\n%cd '/content/gdrive/MyDrive/Colab Notebooks/Challenge'","metadata":{"id":"8xJWfXNdkcWR","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6018282d-ba11-490d-a47b-e510c60269aa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Mounted at /content/gdrive\n\n/content/gdrive/MyDrive/Colab Notebooks/Challenge\n"}]},{"cell_type":"markdown","source":"# 2. Prepare model, data","metadata":{"id":"moZrBbKjUn5K"}},{"cell_type":"code","source":"torch.cuda.empty_cache()\ntorch.cuda.memory_summary(device=device, abbreviated=False)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":338},"id":"MX4tPv2aXDc9","outputId":"a331fca7-8a86-488a-8d58-a7e04a50fd2e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-7-34abe5d5d7a2>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_summary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabbreviated\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/memory.py\u001b[0m in \u001b[0;36mmemory_summary\u001b[0;34m(device, abbreviated)\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0mmanagement\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m     \"\"\"\n\u001b[0;32m--> 491\u001b[0;31m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_device_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptional\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m     \u001b[0mstats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmemory_stats\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/cuda/_utils.py\u001b[0m in \u001b[0;36m_get_device_index\u001b[0;34m(device, optional, allow_cpu)\u001b[0m\n\u001b[1;32m     33\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a cuda or cpu device, but got: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"cuda\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Expected a cuda device, but got: {device}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_scripting\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: Expected a cuda device, but got: cpu"]}]},{"cell_type":"markdown","source":"### 2.0. Load Data","metadata":{"id":"zsjUTFTDTj-G"}},{"cell_type":"code","source":"# Phobert token\ntrain_input_ids = torch.load(\"train_input_ids.pt\")\ntrain_attention_mask = torch.load(\"train_attention_mask.pt\")\ntrain_labels = torch.load(\"train_labels.pt\")\n\ntest_input_ids = torch.load(\"test_input_ids.pt\")\ntest_attention_mask = torch.load(\"test_attention_mask.pt\")","metadata":{"id":"UhMziK7pTnsH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Svd matrix\ntrain_svd_matrix = torch.load(\"train_svd_matrix.pt\")\ntest_svd_matrix = torch.load(\"test_svd_matrix.pt\")","metadata":{"id":"iEfLlTysT37E"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Phobert token\ntrain_input_ids=train_input_ids.cpu().numpy()\ntrain_attention_mask=train_attention_mask.cpu().numpy()\ntest_input_ids=test_input_ids.cpu().numpy()\ntest_attention_mask=test_attention_mask.cpu().numpy()\n# Svd matrix\ntrain_svd_matrix=train_svd_matrix.cpu().numpy()\ntest_svd_matrix=test_svd_matrix.cpu().numpy()","metadata":{"id":"43_9SZAOTq1q"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Phobert token\ntrain_input_ids = torch.from_numpy(train_input_ids)\ntrain_attention_mask = torch.from_numpy(train_attention_mask)\ntest_input_ids = torch.from_numpy(test_input_ids)\ntest_attention_mask = torch.from_numpy(test_attention_mask)\n# Svd matrix\ntrain_svd_matrix = torch.from_numpy(train_svd_matrix)\ntest_svd_matrix = torch.from_numpy(test_svd_matrix)","metadata":{"id":"_FRrRUxfTthN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(train_input_ids.shape)\nprint(train_attention_mask.shape)\nprint(test_input_ids.shape)\nprint(test_attention_mask.shape)\nprint(train_svd_matrix.shape)\nprint(test_svd_matrix.shape)","metadata":{"id":"7yzIn1mBTwLj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Phobert token\n# train_input_ids = train_input_ids.to(device)\n# train_attention_mask = train_attention_mask.to(device)\n# test_input_ids = train_input_ids.to(device)\n# test_attention_mask = train_attention_mask.to(device)\n# Svd matrix\n# train_svd_matrix = train_svd_matrix.to(device)\n# test_svd_matrix = test_svd_matrix.to(device)","metadata":{"id":"e5DB479bVZLy"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.1. Fine-tuning Phobert Model","metadata":{"id":"hAwAVeLAQS77"}},{"cell_type":"code","source":"# Phobert model\nnum_labels = 3\n# tokenizer = PhobertTokenizer.from_pretrained('vinai/phobert-base')\nphobert_model = RobertaModel.from_pretrained('vinai/phobert-base', num_labels=num_labels).to(device)\n\n# model = CNNModel(num_labels,phobert_model ).to(device)\n# transformers_logger = logging.getLogger(\"transformers\")\n# transformers_logger.setLevel(logging.ERROR)\nclass MyPhoBERTModel(nn.Module):\n    def __init__(self, num_labels,phobert_model):\n        super(MyPhoBERTModel, self).__init__()\n        self.phobert = phobert_model\n        self.reduce_mean_layers = nn.Sequential(\n            nn.LayerNorm(768),\n            nn.Linear(768, 768),\n            nn.LayerNorm(768),\n            nn.Linear(768, 768),\n            nn.LayerNorm(768),\n            nn.Linear(768, 768)\n        )\n\n    def forward(self, input_ids, attention_mask):\n        outputs = self.phobert(input_ids=input_ids, attention_mask=attention_mask)\n        last_hidden_states = outputs.last_hidden_state\n\n        # Apply reduce_mean on the last three layers' hidden states\n        reduced_hidden_states = self.reduce_mean_layers(last_hidden_states[:, -4:, :]).mean(dim=1)\n\n        return {\n            'last_hidden_state': reduced_hidden_states\n        }\n\n# Example usage\n\nphobert_model = MyPhoBERTModel(num_labels,phobert_model).to(device)\n# input_ids = tokenizer.encode(\"Hello, how are you?\", add_special_tokens=True, truncation=True, padding='max_length', max_length=64, return_tensors='pt')\n# attention_mask = input_ids.gt(0)\n\n\n# outputs = phobert_model(train_input_ids, train_attention_mask)\n# train_phobert_embeddings = outputs['last_hidden_state']","metadata":{"id":"Ks_xQzPePRPp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"batch_size = 10  # Giá trị batch size nhỏ hơn\n\nnum_samples = train_input_ids.size(0)  # Số lượng mẫu huấn luyện\nnum_batches = num_samples // batch_size  # Số lượng batch\n\ntrain_phobert_embeddings = []  # Danh sách để tích lũy kết quả embeddings\n\nfor i in range(num_batches):\n    start_index = i * batch_size\n    end_index = (i + 1) * batch_size\n\n    batch_input_ids = train_input_ids[start_index:end_index]\n    batch_attention_mask = train_attention_mask[start_index:end_index]\n\n    # Chuyển batch sang GPU (nếu cần)\n    batch_input_ids = batch_input_ids.to(device)\n    batch_attention_mask = batch_attention_mask.to(device)\n\n    # Truyền batch vào mô hình\n    outputs = phobert_model(batch_input_ids, batch_attention_mask)\n    batch_embeddings = outputs['last_hidden_state']\n\n    # Gộp batch_embeddings vào danh sách tổng\n    train_phobert_embeddings.append(batch_embeddings)\n\n# Xử lý batch cuối cùng (nếu có)\nif num_samples % batch_size != 0:\n    start_index = num_batches * batch_size\n    end_index = num_samples\n\n    batch_input_ids = train_input_ids[start_index:end_index]\n    batch_attention_mask = train_attention_mask[start_index:end_index]\n\n    # Chuyển batch sang GPU (nếu cần)\n    batch_input_ids = batch_input_ids.to(device)\n    batch_attention_mask = batch_attention_mask.to(device)\n\n    # Truyền batch vào mô hình\n    outputs = phobert_model(batch_input_ids, batch_attention_mask)\n    batch_embeddings = outputs['last_hidden_state']\n\n    # Gộp batch_embeddings vào danh sách tổng\n    train_phobert_embeddings.append(batch_embeddings)\n\n# Tạo tensor từ danh sách embeddings\n","metadata":{"id":"vOslCOby-YUY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_phobert_embeddings = torch.cat(train_phobert_embeddings, dim=0)","metadata":{"id":"z3lUYf3hhLOs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 2.2. CNN model","metadata":{"id":"BWOnepn7QXry"}},{"cell_type":"code","source":"# Combine PhoBERT embeddings and SVD embeddings\n# combined_embeddings = torch.cat((phobert_embeddings, torch.tensor(svd_matrix)), dim=1)\n\n# Define CNN model\nclass CNNClassifier(nn.Module):\n    def __init__(self, input_dim, num_filters, kernel_sizes, num_classes):\n        super(CNNClassifier, self).__init__()\n        self.convs = nn.ModuleList([nn.Conv1d(in_channels=input_dim, out_channels=num_filters, kernel_size=ks) for ks in kernel_sizes])\n        self.fc = nn.Linear(len(kernel_sizes) * num_filters, num_classes)\n\n    def forward(self, x):\n        x = x.unsqueeze(1)  # Add channel dimension (batch_size, channels, seq_len)\n        x = [F.relu(conv(x)) for conv in self.convs]  # Apply convolution and ReLU activation\n        x = [F.max_pool1d(c, c.size(2)).squeeze(2) for c in x]  # Max pooling\n        x = torch.cat(x, 1)  # Concatenate pooled features from different kernel sizes\n        x = F.dropout(x, p=0.5, training=self.training)  # Apply dropout\n        x = self.fc(x)  # Fully connected layer\n        return x\n\n# Initialize CNN model\n# input_dim = combined_embeddings.size(1)\nnum_filters = 100\nkernel_sizes = [3, 4, 5]\nnum_classes = 3  # Number of verdict classes\nmodel = CNNClassifier(1024, num_filters, kernel_sizes, num_classes)\n\n# # Forward pass\n# outputs = model(combined_embeddings)\n\n# print(outputs)","metadata":{"id":"Shq_p9zuQSEM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Chia dữ liệu thành tập huấn luyện và tập kiểm tra\n\n# Chia tập train thành tập train và tập validation\ntrain_input_ids, val_input_ids, train_attention_mask, val_attention_mask, train_labels, val_labels =train_test_split(train_input_ids, train_attention_mask, train_labels, test_size=0.2, random_state=seed)","metadata":{"id":"WXeU70JwO0uV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Initialization","metadata":{"id":"01zGSMal72TK"}},{"cell_type":"code","source":"max_length = 10\n# Các tham số huấn luyện\nbatch_size = 10\nepochs = 5\nlearning_rate = 2e-5\nshuffle = True\nnum_workers = 2\n# Tạo train dataset từ tập train\ntrain_dataset = TensorDataset(train_input_ids, train_attention_mask, train_labels)\n\n# Tạo validation dataset từ tập validation\nval_dataset = TensorDataset(val_input_ids, val_attention_mask, val_labels)\n\ndef custom_collate(batch):\n    # Trích xuất các thành phần từ batch\n    input_ids_batch, attention_mask_batch, labels_batch = zip(*batch)\n\n    # Đảm bảo các dữ liệu có cùng kích thước\n    max_length = max(len(ids) for ids in input_ids_batch)\n    input_ids_batch = [torch.cat((ids, torch.tensor([1] * (max_length - len(ids)), dtype=torch.long))) for ids in input_ids_batch]\n    attention_mask_batch = [torch.cat((mask, torch.tensor([1] * (max_length - len(mask)), dtype=torch.long))) for mask in attention_mask_batch]\n\n    # Chuyển đổi thành tensor\n    input_ids_batch = torch.stack(input_ids_batch)\n    attention_mask_batch = torch.stack(attention_mask_batch)\n    labels_batch = torch.tensor(labels_batch, dtype=torch.long)\n\n    return input_ids_batch, attention_mask_batch, labels_batch\n\n# Tạo train dataloader từ train dataset và custom collate function\ntrain_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate, num_workers=num_workers, worker_init_fn=np.random.seed(seed))\n\n# Tạo validation dataloader từ validation dataset và custom collate function\nval_dataloader = DataLoader(val_dataset, batch_size=batch_size, collate_fn=custom_collate, shuffle=False, num_workers=num_workers )","metadata":{"id":"00kqxWb0Wsj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# Tạo optimizer và scheduler\noptimizer = AdamW(model.parameters(), lr=learning_rate)\ntotal_steps = len(train_dataloader) * epochs\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)","metadata":{"id":"y5k3Bs1nO6i0","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a3a08201-79bf-49e7-80c1-606b3373615a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n\n  warnings.warn(\n"}]},{"cell_type":"markdown","source":"# Train and Evaluation","metadata":{"id":"6L5Gab6yLw1K"}},{"cell_type":"code","source":"# Huấn luyện mô hình\n# Tính toán độ dài câu tối đa trong batch\n\n\n\nmodel.train()\nfor epoch in range(epochs):\n    total_loss = 0\n    for batch in train_dataloader:\n        input_ids = batch[0].to(torch.long)\n\n        attention_mask = batch[1].to(torch.long)\n        labels = batch[2].to(torch.long)\n        input_ids= input_ids.to(device)\n        attention_mask= attention_mask.to(device)\n        labels= labels.to(device)\n\n        optimizer.zero_grad()\n\n        logits = model(input_ids, attention_mask)\n        loss_fn = nn.CrossEntropyLoss()\n        loss = loss_fn(logits, labels)\n        # outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n        # loss = outputs.loss\n\n        total_loss += loss.item()\n\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n\n        optimizer.step()\n        scheduler.step()\n\n    average_loss = total_loss / len(train_dataloader)\n    print(f\"Epoch {epoch+1}/{epochs} - Average Loss: {average_loss}\")\n","metadata":{"id":"B1f0z7e5O-eL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Đánh giá mô hình trên tập kiểm tra\nmodel.eval()\npredictions = []\nwith torch.no_grad():\n    for batch in val_dataloader:\n        input_ids = batch[0].to(torch.long)\n        attention_mask = batch[1].to(torch.long)\n        labels = batch[2].to(torch.long)\n        input_ids= input_ids.to(device)\n        attention_mask= attention_mask.to(device)\n        labels= labels.to(device)\n\n        logits = model(input_ids, attention_mask)\n        _, predicted_classes = torch.max(logits, dim=1)\n\n        predictions.extend(predicted_classes.cpu().numpy())\n","metadata":{"id":"tTMtU-3RO_OT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Tính toán độ chính xác và các độ đo đánh giá khác\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n\naccuracy = accuracy_score(val_labels, predictions)\nprecision = precision_score(val_labels, predictions, average=\"weighted\")\nrecall = recall_score(val_labels, predictions, average=\"weighted\")\nf1 = f1_score(val_labels, predictions, average=\"weighted\")\n\nprint(f\"Accuracy: {accuracy}\")\nprint(f\"Precision: {precision}\")\nprint(f\"Recall: {recall}\")\nprint(f\"F1-Score: {f1}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4FoV-onHXXii","outputId":"91b4c94a-a0bf-4140-a8bb-b8749264a2b2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":"Accuracy: 0.768075639599555\n\nPrecision: 0.7740248049812857\n\nRecall: 0.768075639599555\n\nF1-Score: 0.767352098980306\n"}]},{"cell_type":"markdown","source":"# Prediction Labels","metadata":{"id":"tmSiDG8FL3A6"}},{"cell_type":"code","source":"test_input_ids = torch.load(\"test_input_ids.pt\")\ntest_attention_mask = torch.load(\"test_attention_mask.pt\")","metadata":{"id":"Xorrxa6B9Xvw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input_ids=test_input_ids.cpu().numpy()\ntest_attention_mask=test_attention_mask.cpu().numpy()","metadata":{"id":"AY2ukpHT9aQY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_input_ids = torch.from_numpy(test_input_ids)\ntest_attention_mask = torch.from_numpy(test_attention_mask)","metadata":{"id":"oshZXBX99moX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_dataset = TensorDataset(test_input_ids, test_attention_mask)\n\n# Tạo DataLoader\n\ntest_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, worker_init_fn=np.random.seed(seed))","metadata":{"id":"EHvIxnlG99kW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_labels","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b4w-3WFa_Jr9","outputId":"107509a2-4588-4a6e-985e-c594ef1ab0d8"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":29,"data":{"text/plain":["tensor([0, 2, 1,  ..., 1, 1, 2])"]},"metadata":{}}]},{"cell_type":"code","source":"\n# Chạy mô hình trên dữ liệu kiểm tra\nmodel.eval()  # Chuyển sang chế độ đánh giá\npredictions_test = []\nwith torch.no_grad():\n    for batch in test_dataloader:\n        input_ids = batch[0].to(torch.long)\n        attention_mask = batch[1].to(torch.long)\n        input_ids= input_ids.to(device)\n        attention_mask= attention_mask.to(device)\n\n        logits = model(input_ids, attention_mask)\n        _, predicted_labels = torch.max(logits, 1)\n        predictions_test.extend(predicted_labels.cpu().numpy())\n","metadata":{"id":"7MsqQTeK9Kod"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predicted_labels = [prediction for prediction in predictions_test]","metadata":{"id":"H7yFqjBv_QSd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\npredicted_labels  = torch.tensor(predicted_labels)\npredicted_labels","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"p7uHTnvDALQu","outputId":"49f6d099-d2fe-43ef-c0fb-6431ae5373b8"},"execution_count":null,"outputs":[{"output_type":"execute_result","execution_count":33,"data":{"text/plain":["tensor([1, 0, 2,  ..., 1, 0, 2])"]},"metadata":{}}]},{"cell_type":"code","source":"day = 20\ntorch.save(predicted_labels, day+\"_predicted_verdicts_labels.pt\")","metadata":{"id":"TFJgFmMnBouN"},"execution_count":null,"outputs":[]}]}